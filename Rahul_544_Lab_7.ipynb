{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-727/NLP-Lab-work/blob/main/Rahul_544_Lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Find the similarity between two documents\n",
        "\n",
        "\n",
        "\n",
        "*   Cosine Similarity measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. In the context of text analysis, these vectors often represent the frequency of occurrence of terms within the documents (Term Frequency or TF-IDF vectors). A cosine similarity of 1 means the documents are identical, while a cosine similarity of 0 indicates no similarity.\n",
        "\n"
      ],
      "metadata": {
        "id": "f9rqhDx0sKhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26s_g25XMRAm",
        "outputId": "ab2e0c11-acfe-4f59-bddf-90e7cab7d829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between the two documents: 0.4356132262843411\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "with open('/content/text1.txt', 'r', encoding='utf-8') as file:\n",
        "    doc1 = file.read()\n",
        "\n",
        "with open('/content/text2.txt', 'r', encoding='utf-8') as file:\n",
        "    doc2 = file.read()\n",
        "\n",
        "documents = [doc1, doc2]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
        "print(\"Cosine Similarity between the two documents:\", cos_sim[0, 1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for Term Frequency-Inverse Document Frequency. It's a numerical representation of a document (a piece of text) that captures the importance of each word within that document relative to a collection of documents.\n",
        "\n",
        "Term Frequency (TF): This measures how often a word appears in a document. If a word appears more frequently in a document, its TF value will be higher.\n",
        "\n",
        "Inverse Document Frequency (IDF): This measures how unique or rare a word is across all the documents in the collection. If a word appears in many documents, its IDF value will be lower."
      ],
      "metadata": {
        "id": "RPABlsMrNXT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Jaccard Similarity measures the similarity between two sets. It's calculated as the size of the intersection divided by the size of the union of two sets. For text analysis, documents are converted into sets of tokens. In simple terms, it tells us how similar or different two sets are by considering the intersection (common elements) and union (total elements) of the sets."
      ],
      "metadata": {
        "id": "hFPfGsV4tTUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(doc1, doc2):\n",
        "    set1 = set(doc1.split())\n",
        "    set2 = set(doc2.split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "jac_sim = jaccard_similarity(doc1, doc2)\n",
        "print(\"Jaccard Similarity between the two documents:\", jac_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwoJFEQzNYkJ",
        "outputId": "b0b490b2-2b9b-45c6-a0e1-d35d83268692"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity between the two documents: 0.09649122807017543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5piPm5-2tmpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Implement the Sentiment Analysis using Bayesian Classification."
      ],
      "metadata": {
        "id": "Nzo1gxQntmsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xnK5K-bRyj-",
        "outputId": "9fcdc3af-5657-4841-9bb9-b301b1f40255"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baeysian Classification\n",
        "\n",
        "Bayesian Classification is a method used in machine learning for predicting the category or class of a given data point based on the probability that it belongs to each category. It's based on Bayes' theorem, which is a fundamental concept in probability theory.\n",
        "\n",
        "* in this we are analyzing the sentiment of that statement whther it is +ve or -ve"
      ],
      "metadata": {
        "id": "xn3wfyqEV7gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "df = pd.read_csv('/content/Tweets.csv')\n",
        "\n",
        "dataset = df[['text', 'airline_sentiment']]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_freq = defaultdict(lambda: [0, 0])\n",
        "for _, row in dataset.iterrows():\n",
        "    text = row['text']\n",
        "    label = row['airline_sentiment']\n",
        "    words = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]\n",
        "    for word in words:\n",
        "        word_freq[word][label == 'positive'] += 1\n",
        "\n",
        "total_positive = sum(word_freq[word][1] for word in word_freq)\n",
        "total_negative = sum(word_freq[word][0] for word in word_freq)\n",
        "prior_positive = total_positive / (total_positive + total_negative)\n",
        "prior_negative = total_negative / (total_positive + total_negative)\n",
        "\n",
        "def classify(text):\n",
        "    words = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]\n",
        "    log_prob_positive = sum([word_freq[word][1] / total_positive for word in words])\n",
        "    log_prob_negative = sum([word_freq[word][0] / total_negative for word in words])\n",
        "    prob_positive = prior_positive * log_prob_positive\n",
        "    prob_negative = prior_negative * log_prob_negative\n",
        "    return 'positive' if prob_positive > prob_negative else 'negative'\n",
        "\n",
        "test_data = [\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA\"]\n",
        "for text in test_data:\n",
        "    sentiment = classify(text)\n",
        "    print(f\"Sentiment of '{text}': {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iBM1vigPCdo",
        "outputId": "58b0b492-c3b8-4bbb-8b50-538a4714060b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of '@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA': negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Implement the Sentiment Analysis using RNN.\n",
        "\n",
        "* RNNs are a class of artificial neural networks designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or numerical time series data. Unlike traditional neural networks, RNNs have loops within them, allowing information to persist. This looped network architecture enables RNNs to take not just the current input but also what they have perceived previously in time into account, making them powerful for sequential data analysis like language processing."
      ],
      "metadata": {
        "id": "Ukc28Xf8WAHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "9J2sYaNVe9a2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Tweets.csv')"
      ],
      "metadata": {
        "id": "8BmyoAtMfAv1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['text'].tolist()\n",
        "labels = df['airline_sentiment'].tolist()\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "labels = df['airline_sentiment'].tolist()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "labels_one_hot = to_categorical(labels_encoded)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "sequences_padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences_padded, labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=max_len))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(3, activation='softmax'))  # Use 3 neurons in the output layer, with a softmax activation function\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)  # Adjust batch size for efficiency\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AFVfQK5Ut2P",
        "outputId": "dd49bd4c-5e54-4ab3-f813-47f6349c6d4b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "366/366 [==============================] - 19s 47ms/step - loss: 0.7800 - accuracy: 0.6678\n",
            "Epoch 2/10\n",
            "366/366 [==============================] - 4s 11ms/step - loss: 0.5032 - accuracy: 0.7982\n",
            "Epoch 3/10\n",
            "366/366 [==============================] - 3s 9ms/step - loss: 0.3566 - accuracy: 0.8726\n",
            "Epoch 4/10\n",
            "366/366 [==============================] - 3s 8ms/step - loss: 0.2493 - accuracy: 0.9159\n",
            "Epoch 5/10\n",
            "366/366 [==============================] - 3s 7ms/step - loss: 0.1825 - accuracy: 0.9442\n",
            "Epoch 6/10\n",
            "366/366 [==============================] - 2s 7ms/step - loss: 0.1469 - accuracy: 0.9554\n",
            "Epoch 7/10\n",
            "366/366 [==============================] - 3s 8ms/step - loss: 0.1134 - accuracy: 0.9674\n",
            "Epoch 8/10\n",
            "366/366 [==============================] - 2s 5ms/step - loss: 0.0989 - accuracy: 0.9728\n",
            "Epoch 9/10\n",
            "366/366 [==============================] - 2s 6ms/step - loss: 0.0855 - accuracy: 0.9764\n",
            "Epoch 10/10\n",
            "366/366 [==============================] - 3s 7ms/step - loss: 0.0727 - accuracy: 0.9798\n",
            "92/92 [==============================] - 1s 3ms/step - loss: 1.0179 - accuracy: 0.7616\n",
            "Test Loss: 1.017911434173584\n",
            "Test Accuracy: 0.7616119980812073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Implement the Sentiment Analysis using LSTM.\n",
        "\n",
        "* Long Short-Term Memory (LSTM) networks, a specific type of Recurrent Neural Network (RNN), is a highly effective approach for analyzing the sentiment of text data due to LSTMs' ability to capture long-term dependencies. This makes LSTMs particularly adept at understanding the nuanced context of language, which is crucial for accurately determining sentiment. Hereâ€™s a non-code overview of how sentiment analysis with LSTM works.\n",
        "* LSTMs are designed to address the vanishing gradient problem of traditional RNNs, allowing them to learn and remember over long sequences of data without losing context or meaning"
      ],
      "metadata": {
        "id": "AYNukyV1fqLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, SpatialDropout1D\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "metadata": {
        "id": "VFjYXzNkier9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/Tweets.csv\")"
      ],
      "metadata": {
        "id": "DVJaTFOeigE_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['text']\n",
        "y = pd.get_dummies(data['airline_sentiment']).values\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(X, maxlen=200)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 128, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORZLaug7OEUp",
        "outputId": "6071d55e-0763-4a32-ed6b-bd7525617a69"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "183/183 [==============================] - 78s 415ms/step - loss: 0.7240 - accuracy: 0.7056 - val_loss: 0.5311 - val_accuracy: 0.7910\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 70s 384ms/step - loss: 0.4679 - accuracy: 0.8180 - val_loss: 0.4964 - val_accuracy: 0.8005\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 68s 373ms/step - loss: 0.3816 - accuracy: 0.8556 - val_loss: 0.5135 - val_accuracy: 0.8016\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 68s 370ms/step - loss: 0.3203 - accuracy: 0.8794 - val_loss: 0.5125 - val_accuracy: 0.8098\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.8963Restoring model weights from the end of the best epoch: 2.\n",
            "183/183 [==============================] - 69s 376ms/step - loss: 0.2833 - accuracy: 0.8963 - val_loss: 0.5360 - val_accuracy: 0.8012\n",
            "Epoch 5: early stopping\n",
            "Test Accuracy: 0.8005464673042297\n"
          ]
        }
      ]
    }
  ]
}