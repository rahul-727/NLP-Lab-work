{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAbCxsf0cYwhQNG2G7qOlE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-727/NLP-Lab-work/blob/main/Rahul_544_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conversational AI\n",
        "\n",
        "In the ever-evolving world of technology, conversational AI ü§ñüó£Ô∏è has taken a giant leap forward. Whether it's for customer service, personal assistants, or interactive entertainment, these AIs have become an integral part of our daily lives. They understand not just the words we say but the nuances behind them, including sarcasm, jokes, and cultural references. However, they don‚Äôt always get it right, which can lead to some funny üòÇ or frustrating üò§ moments. Despite these challenges, the advancements in natural language processing (NLP) have made interactions more seamless and human-like than ever before."
      ],
      "metadata": {
        "id": "p7yDWDI2mRVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84DS7c9vpo9e",
        "outputId": "5e1fb958-1c26-4e7f-f456-c85f5a89fd33"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#a.\tWord Tokenization\n",
        "\n",
        "Splits a sentence into words using spaces and punctuation as delimiters.\n",
        "\n",
        "Example: \"Hello, world!\" ‚Üí [\"Hello\", \",\", \"world\", \"!\"]"
      ],
      "metadata": {
        "id": "cUKK1T7Mmb5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S68-DNCumQX3",
        "outputId": "374f185f-ee1b-48df-94ba-9fc7bb4328f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational', 'AI', ',', 'transforming', 'how', 'we', 'interact', '!', 'üòäüöÄ', 'From', 'simple', 'Q', '&', 'A', 'to', 'complex', 'dialogues', '.', 'In', 'the', 'ever-evolving', 'world', 'of', 'technology', ',', 'conversational', 'AI', 'ü§ñüó£Ô∏è', 'has', 'taken', 'a', 'giant', 'leap', 'forward', '.', 'Whether', 'it', \"'s\", 'for', 'customer', 'service', ',', 'personal', 'assistants', ',', 'or', 'interactive', 'entertainment', ',', 'these', 'AIs', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives', '.', 'They', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', ',', 'including', 'sarcasm', ',', 'jokes', ',', 'and', 'cultural', 'references', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Conversational AI, transforming how we interact! üòäüöÄ From simple Q&A to complex dialogues. In the ever-evolving world of technology, conversational AI ü§ñüó£Ô∏è has taken a giant leap forward. Whether it's for customer service, personal assistants, or interactive entertainment, these AIs have become an integral part of our daily lives. They understand not just the words we say but the nuances behind them, including sarcasm, jokes, and cultural references.\"\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#b.\tSentence Tokenization\n",
        " Splits a text into sentences using punctuation and capitalization as hints.\n",
        "\n",
        "Example: \"First sentence. Second sentence.\" ‚Üí [\"First sentence.\", \"Second sentence.\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "vIbVvL7-nT3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-u_UESnnXAf",
        "outputId": "940a060b-847a-44ab-f133-cd7bc064b9a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational AI, transforming how we interact!', 'üòäüöÄ From simple Q&A to complex dialogues.', 'In the ever-evolving world of technology, conversational AI ü§ñüó£Ô∏è has taken a giant leap forward.', \"Whether it's for customer service, personal assistants, or interactive entertainment, these AIs have become an integral part of our daily lives.\", 'They understand not just the words we say but the nuances behind them, including sarcasm, jokes, and cultural references.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#c. Punctuation-based Tokenizer\n",
        "Splits text based on a regular expression that might prioritize punctuation.\n",
        "\n",
        "Example: \"Money ($100) is not everything.\" ‚Üí [\"Money\", \"(\", \"$100\", \")\", \"is\", \"not\", \"everything\", \".\"]"
      ],
      "metadata": {
        "id": "EPNgceyNn20W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "punct_tokenizer = WordPunctTokenizer()\n",
        "punct_tokens = punct_tokenizer.tokenize(text)\n",
        "print(punct_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRdQO3BznxPP",
        "outputId": "30cfc25b-c925-4a7e-82e9-6f22d8ee0ecd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational', 'AI', ',', 'transforming', 'how', 'we', 'interact', '!', 'üòäüöÄ', 'From', 'simple', 'Q', '&', 'A', 'to', 'complex', 'dialogues', '.', 'In', 'the', 'ever', '-', 'evolving', 'world', 'of', 'technology', ',', 'conversational', 'AI', 'ü§ñüó£Ô∏è', 'has', 'taken', 'a', 'giant', 'leap', 'forward', '.', 'Whether', 'it', \"'\", 's', 'for', 'customer', 'service', ',', 'personal', 'assistants', ',', 'or', 'interactive', 'entertainment', ',', 'these', 'AIs', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives', '.', 'They', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', ',', 'including', 'sarcasm', ',', 'jokes', ',', 'and', 'cultural', 'references', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#d. Treebank Word tokenizer\n",
        "\n",
        "Uses regular expressions to tokenize text in a way that's similar to the Penn Treebank.\n",
        "\n",
        "Example: \"I'm happy.\" ‚Üí [\"I\", \"'m\", \"happy\", \".\"]"
      ],
      "metadata": {
        "id": "nldFDn1ZoA0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "print(treebank_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmgOvSifoANJ",
        "outputId": "0f15ddac-17b7-4a0f-be16-9c15d9e7210e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational', 'AI', ',', 'transforming', 'how', 'we', 'interact', '!', 'üòäüöÄ', 'From', 'simple', 'Q', '&', 'A', 'to', 'complex', 'dialogues.', 'In', 'the', 'ever-evolving', 'world', 'of', 'technology', ',', 'conversational', 'AI', 'ü§ñüó£Ô∏è', 'has', 'taken', 'a', 'giant', 'leap', 'forward.', 'Whether', 'it', \"'s\", 'for', 'customer', 'service', ',', 'personal', 'assistants', ',', 'or', 'interactive', 'entertainment', ',', 'these', 'AIs', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives.', 'They', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', ',', 'including', 'sarcasm', ',', 'jokes', ',', 'and', 'cultural', 'references', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#e. Tweet Tokenizer\n",
        "NLTK's TweetTokenizer: Specialized for the nuances of social media texts, handling emojis, hashtags, and mentions effectively.\n",
        "\n",
        "Example: \"Loving this! üòç #awesome\" ‚Üí [\"Loving\", \"this\", \"!\", \"üòç\", \"#awesome\"]"
      ],
      "metadata": {
        "id": "0R1ZlWVVoMf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tweet_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poevipAGoMxy",
        "outputId": "d2b8eef0-3843-4261-ac70-e1e5a2662808"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational', 'AI', ',', 'transforming', 'how', 'we', 'interact', '!', 'üòä', 'üöÄ', 'From', 'simple', 'Q', '&', 'A', 'to', 'complex', 'dialogues', '.', 'In', 'the', 'ever-evolving', 'world', 'of', 'technology', ',', 'conversational', 'AI', 'ü§ñ', 'üó£', 'Ô∏è', 'has', 'taken', 'a', 'giant', 'leap', 'forward', '.', 'Whether', \"it's\", 'for', 'customer', 'service', ',', 'personal', 'assistants', ',', 'or', 'interactive', 'entertainment', ',', 'these', 'AIs', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives', '.', 'They', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', ',', 'including', 'sarcasm', ',', 'jokes', ',', 'and', 'cultural', 'references', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#f. Multi-Word Expression Tokenizer\n",
        "NLTK's MWETokenizer: Tokenizes text into single tokens, combining multi-word expressions into single tokens.\n",
        "\n",
        "Example: \"The United States is big.\" (with \"United States\" as a MWE) ‚Üí [\"The\", \"United_States\", \"is\", \"big\", \".\"]"
      ],
      "metadata": {
        "id": "9BOAgMY2obcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe_tokenizer = MWETokenizer([('Conversational', 'AI')], separator='_')\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
        "print(mwe_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL9uY6rNobyR",
        "outputId": "aad6360a-646d-4d72-8a7c-84e7dc8ea6bb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational_AI', ',', 'transforming', 'how', 'we', 'interact', '!', 'üòäüöÄ', 'From', 'simple', 'Q', '&', 'A', 'to', 'complex', 'dialogues', '.', 'In', 'the', 'ever-evolving', 'world', 'of', 'technology', ',', 'conversational', 'AI', 'ü§ñüó£Ô∏è', 'has', 'taken', 'a', 'giant', 'leap', 'forward', '.', 'Whether', 'it', \"'s\", 'for', 'customer', 'service', ',', 'personal', 'assistants', ',', 'or', 'interactive', 'entertainment', ',', 'these', 'AIs', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives', '.', 'They', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', ',', 'including', 'sarcasm', ',', 'jokes', ',', 'and', 'cultural', 'references', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#g. TextBlob Word Tokenize\n",
        "TextBlob's tokenizer: Provides easy-to-use tokenization, handling words and sentences.\n",
        "\n",
        "Example: \"TextBlob is simple.\" ‚Üí [\"TextBlob\", \"is\", \"simple\"]"
      ],
      "metadata": {
        "id": "aWWXcbltojuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "blob = TextBlob(text)\n",
        "textblob_tokens = blob.words\n",
        "print(textblob_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnhkmW3Noj7w",
        "outputId": "8db30eb1-69f9-460f-f8aa-bd2b2c26d632"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational', 'AI', 'transforming', 'how', 'we', 'interact', 'üòäüöÄ', 'From', 'simple', 'Q', 'A', 'to', 'complex', 'dialogues', 'In', 'the', 'ever-evolving', 'world', 'of', 'technology', 'conversational', 'AI', 'ü§ñüó£Ô∏è', 'has', 'taken', 'a', 'giant', 'leap', 'forward', 'Whether', 'it', \"'s\", 'for', 'customer', 'service', 'personal', 'assistants', 'or', 'interactive', 'entertainment', 'these', 'AIs', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives', 'They', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', 'including', 'sarcasm', 'jokes', 'and', 'cultural', 'references']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#h. spaCy Tokenizer\n",
        "spaCy's tokenizer: Highly efficient and accurate tokenizer that is part of the spaCy NLP library.\n",
        "\n",
        "Example: \"spaCy excels at NLP.\" ‚Üí [\"spaCy\", \"excels\", \"at\", \"NLP\"]"
      ],
      "metadata": {
        "id": "1gkMD2-KoyN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(spacy_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bup3ymKAoyiC",
        "outputId": "246c2b05-07ff-43f7-910d-1a0c6f26047f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational', 'AI', ',', 'transforming', 'how', 'we', 'interact', '!', 'üòä', 'üöÄ', 'From', 'simple', 'Q&A', 'to', 'complex', 'dialogues', '.', 'In', 'the', 'ever', '-', 'evolving', 'world', 'of', 'technology', ',', 'conversational', 'AI', 'ü§ñ', 'üó£', 'Ô∏è', 'has', 'taken', 'a', 'giant', 'leap', 'forward', '.', 'Whether', 'it', \"'s\", 'for', 'customer', 'service', ',', 'personal', 'assistants', ',', 'or', 'interactive', 'entertainment', ',', 'these', 'AIs', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives', '.', 'They', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', ',', 'including', 'sarcasm', ',', 'jokes', ',', 'and', 'cultural', 'references', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#i. Gensim word tokenizer\n",
        "Gensim's simple_preprocess: Simplifies text to lowercase and tokenizes, mainly for topic modeling.\n",
        "\n",
        "Example: \"Gensim handles text well.\" ‚Üí [\"gensim\", \"handles\", \"text\", \"well\"]"
      ],
      "metadata": {
        "id": "gi_pY6beo7Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "gensim_tokens = list(tokenize(text))\n",
        "print(gensim_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16c3kHE5o7Y3",
        "outputId": "95db833c-38eb-4d7a-9dbd-8463dfac58bf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Conversational', 'AI', 'transforming', 'how', 'we', 'interact', 'From', 'simple', 'Q', 'A', 'to', 'complex', 'dialogues', 'In', 'the', 'ever', 'evolving', 'world', 'of', 'technology', 'conversational', 'AI', 'has', 'taken', 'a', 'giant', 'leap', 'forward', 'Whether', 'it', 's', 'for', 'customer', 'service', 'personal', 'assistants', 'or', 'interactive', 'entertainment', 'these', 'AIs', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives', 'They', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', 'including', 'sarcasm', 'jokes', 'and', 'cultural', 'references']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#j. Tokenization with Keras\n",
        "Keras's text_to_word_sequence: Tokenizes into words, lowercasing and splitting on spaces.\n",
        "\n",
        "Example: \"Keras makes ML easy.\" ‚Üí [\"keras\", \"makes\", \"ml\", \"easy\"]"
      ],
      "metadata": {
        "id": "gUAHNdSJpEV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "keras_tokens = text_to_word_sequence(text)\n",
        "print(keras_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IperClVpEst",
        "outputId": "9e4d2421-5534-46d8-f558-04658a69a24b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['conversational', 'ai', 'transforming', 'how', 'we', 'interact', 'üòäüöÄ', 'from', 'simple', 'q', 'a', 'to', 'complex', 'dialogues', 'in', 'the', 'ever', 'evolving', 'world', 'of', 'technology', 'conversational', 'ai', 'ü§ñüó£Ô∏è', 'has', 'taken', 'a', 'giant', 'leap', 'forward', 'whether', \"it's\", 'for', 'customer', 'service', 'personal', 'assistants', 'or', 'interactive', 'entertainment', 'these', 'ais', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'daily', 'lives', 'they', 'understand', 'not', 'just', 'the', 'words', 'we', 'say', 'but', 'the', 'nuances', 'behind', 'them', 'including', 'sarcasm', 'jokes', 'and', 'cultural', 'references']\n"
          ]
        }
      ]
    }
  ]
}